# Experimental Rationale

1) This project is focused on studying geometry-in-depth more than just finding a model which will provide best accuracy. So we embarked on a series of experiments to unearth geometric representation in various scenarios – which are listed below.
2) The baseline model for geometric representation is Linear SVC but we consciously included Logistic regression. Linear SVC is margin based while Logistic Regression is likelihood based; the fact that both performed similarly suggests that the dataset is not highly sensitive to boundary philosophy.
3) During PCA transformation, we had picked up 8 principal components which gave maximum accuracy compared to 4 PC and 12 PC. We had to pick up this one quite consciously since we observed that variance was distributed across features and no obvious redundancy was visible among the 30 features.
4) In order to see the visual impact of PCA, we chose only the first 2 principal components deliberately which constituted only 62% of the overall variation. We compared the PCA plot against the picture of manually chosen features which can explain 90% of deviation. Very clearly the latter showed more skewness compared to the PCA plot but when we tried SVC model upon both these variations, we could see the classification is far from accurate under both the models.
5) We also tried to swap the order of tasks – to perform PCA first before Train / Test split and compare the results against the standard process of splitting and then applying PCA. In this dataset, applying PCA before the train–test split did not materially change performance, suggesting limited sensitivity to that specific misuse under current conditions.
6) While trying out non-linear kernels, we tried to compare the models with optimized principal components as well as models without PCA. It was interesting to note that Linear SVC with PCA components provided an accuracy of 0.9825 & only 1 False Negative while RBF SVC without PCA achieved performance close to Linear SVC with PCA (0.9737 accuracy with nil false negatives). However, RBF SVC needed approximately three times the number of support vectors that Linear SVC required.
7) We introduced both the Polynomial and RBF kernel to study the geometric behaviour – former to test global nonlinear interactions and latter to test the local similarity structure. We found that Polynomial under-performed while RBF kernel without PCs brought results close to Linear SVC with PC, suggesting that the underlying geometry is largely compatible with linear separation once reasonably conditioned.
8) Between the two non-linear kernels, the impact of PCA was another interesting observation. We found that neither of them drastically improved the results after the PCA transformation. We neither discovered any interaction-driven structure (if Polynomial had drastically improved) nor discovered cluster-driven non-linearity (in case of substantial improvement in RBF).
9) When we tried the perturbed dataset to have only 15% of Malignant records (against original 37%), we found that the imbalance made the problem appear easier without fundamentally increasing geometric structure. Kernel methods did not uncover hidden nonlinear separability; instead, boundary complexity adjusted as class tension reduced. (More details on perturbed dataset experiment covered in Appendix.)
